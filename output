Concrete ML Open Zama Library:-

Training on  cuda
Loading and preprocessing Spambase dataset...
Training Logistic Regression with Concrete ML...
Evaluating Logistic Regression on clear data (non-FHE)...
Clear Test Accuracy (Logistic Regression): 90.55%
Compiling Logistic Regression for FHE...
Generating a key for an 38-bit circuit
Key generation time: 0.0001 seconds
Running FHE Inference for Logistic Regression...
Execution time: 0.0151 seconds per sample
FHE Test Accuracy (Logistic Regression): 90.55%
Comparing clear and FHE predictions for Logistic Regression...
Number of prediction mismatches: 0
Training MLP with Concrete ML...
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        0.6728       0.6128        0.6708  1.7342
      2        0.6678       0.6128        0.6688  1.7567
      3        0.6670       0.6128        0.6648  1.6643
      4        0.6615       0.6128        0.6568  1.6500
      5        0.6574       0.6128        0.6555  1.8431
      6        0.6539       0.6128        0.6465  1.7257
      7        0.6469       0.6128        0.6429  3.3186
      8        0.6378       0.6128        0.6268  2.6693
      9        0.6332       0.6128        0.6326  1.8476
     10        0.6570       0.6128        0.6674  1.6992
     11        0.6620       0.6128        0.6484  1.6107
     12        0.6604       0.6128        0.6638  1.7749
     13        0.6576       0.6128        0.6402  1.8261
     14        0.6553       0.6128        0.6633  1.7354
     15        0.6562       0.6128        0.6633  1.7155
     16        0.6578       0.6128        0.6633  1.7744
     17        0.6493       0.6128        0.6350  1.8372
     18        0.6478       0.6128        0.6313  1.6449
     19        0.6421       0.6128        0.6323  1.6660
     20        0.6346       0.6128        0.6026  1.7330
     21        0.6211       0.6902        0.6064  3.1971
     22        0.6189       0.6141        0.6272  3.6123
     23        0.6162       0.7242        0.5758  3.6142
     24        0.5981       0.7160        0.5560  3.6375
     25        0.5826       0.7052        0.5710  3.6060
     26        0.5856       0.7106        0.5727  3.6444
     27        0.6110       0.7201        0.5645  3.8300
     28        0.5926       0.6576        0.5533  3.6846
     29        0.5914       0.7174        0.5488  3.6511
     30        0.5715       0.7459        0.5222  3.5945
     31        0.5765       0.7242        0.5156  3.9789
     32        0.5692       0.7649        0.5105  2.8833
     33        0.5601       0.7704        0.5081  3.7092
     34        0.5656       0.7758        0.5075  3.7865
     35        0.5890       0.7378        0.5626  3.0509
     36        0.5976       0.7582        0.5487  3.1101
     37        0.6154       0.6318        0.6402  2.4295
     38        0.6095       0.6182        0.6588  2.0272
     39        0.6131       0.7459        0.5516  1.9366
     40        0.6067       0.7283        0.5291  4.0439
     41        0.6046       0.6236        0.6475  4.0502
     42        0.5979       0.7310        0.5383  2.8803
     43        0.5920       0.7418        0.5478  2.3866
     44        0.5929       0.7378        0.5361  3.7818
     45        0.5945       0.7622        0.5865  3.6610
     46        0.5905       0.7649        0.5293  2.9902
     47        0.5897       0.7554        0.5394  1.7941
     48        0.6237       0.7541        0.5876  3.2227
     49        0.6022       0.7636        0.5377  4.1730
     50        0.6118       0.7473        0.5467  3.9150
     51        0.6197       0.7188        0.5446  3.8389
     52        0.6124       0.7364        0.5266  3.7313
     53        0.6069       0.4769        0.7464  3.7863
     54        0.5964       0.7554        0.5296  3.9308
     55        0.6066       0.6141        0.6568  2.8619
     56        0.6281       0.7785        0.5539  1.8787
     57        0.6208       0.7255        0.6147  1.7400
     58        0.6180       0.7867        0.5266  1.6587
     59        0.5892       0.7622        0.5113  1.5703
     60        0.6315       0.6155        0.6665  1.9007
     61        0.6207       0.7745        0.6210  3.0746
     62        0.6133       0.7391        0.5346  3.8331
     63        0.6227       0.7432        0.5246  3.7390
     64        0.6156       0.6399        0.6052  3.8549
     65        0.6218       0.7418        0.5770  4.2420
     66        0.6218       0.6467        0.6146  3.7472
     67        0.6242       0.6128        0.6657  3.8875
     68        0.6299       0.8016        0.5299  3.3546
     69        0.6266       0.6128        0.6605  2.9679
     70        0.6327       0.6671        0.5776  1.7494
     71        0.6332       0.6155        0.6617  3.5244
     72        0.6398       0.6236        0.6330  3.8606
     73        0.6344       0.6916        0.5470  3.8756
     74        0.6294       0.6753        0.5655  3.0964
     75        0.6273       0.6087        0.6619  6.7662
     76        0.6261       0.6413        0.6101  5.0785
     77        0.6376       0.6603        0.6046  4.7093
     78        0.6440       0.6128        0.6514  3.7946
     79        0.6400       0.6454        0.6171  4.6172
     80        0.6353       0.5870        0.6406  4.8321
     81        0.6358       0.6304        0.6198  4.9070
     82        0.6314       0.6128        0.6788  4.8262
     83        0.6318       0.7731        0.5782  4.7520
     84        0.6284       0.6141        0.6857  4.6935
     85        0.6338       0.6427        0.5811  4.8115
     86        0.6146       0.6671        0.5891  4.5072
     87        0.6416       0.6318        0.6375  4.5478
     88        0.6256       0.7785        0.5550  4.8684
     89        0.6285       0.6128        0.6811  4.9878
     90        0.6337       0.6630        0.5987  4.7825
     91        0.6314       0.6128        0.6741  5.3508
     92        0.6255       0.6644        0.6078  5.2171
     93        0.6091       0.7840        0.4953  4.6820
     94        0.6162       0.7799        0.5270  4.9840
     95        0.6221       0.7147        0.5996  5.8798
     96        0.6362       0.7500        0.5756  5.8716
     97        0.6381       0.6141        0.6556  5.0854
     98        0.6328       0.6128        0.6827  3.9365
     99        0.6336       0.6739        0.5985  4.9423
    100        0.6182       0.7758        0.5322  5.0984
Clear Test Accuracy (Logistic Regression): 57.98%
Compiling MLP for FHE...
Generating a key for an 6-bit circuit
Key generation time: 1.2783 seconds
Running FHE Inference for Multi-Layer Perceptron (MLP)...
Execution time: 1.2475 seconds per sample
FHE Test Accuracy (Multi-Layer Perceptron (MLP)): 57.98%
Comparing clear and FHE predictions for Multi-Layer Perceptron (MLP)...
Number of prediction mismatches: 0

Normal de-encryted training(using Pytorch):-
Training on  cuda
Loading and preprocessing Spambase dataset...
Training Logistic Regression...
Epoch 1/100, Loss: 0.6546, Accuracy: 61.33%
Epoch 2/100, Loss: 0.6285, Accuracy: 62.64%
Epoch 3/100, Loss: 0.6043, Accuracy: 65.00%
Epoch 4/100, Loss: 0.5846, Accuracy: 68.70%
Epoch 5/100, Loss: 0.5678, Accuracy: 70.95%
Epoch 6/100, Loss: 0.5537, Accuracy: 73.67%
Epoch 7/100, Loss: 0.5404, Accuracy: 75.49%
Epoch 8/100, Loss: 0.5288, Accuracy: 77.55%
Epoch 9/100, Loss: 0.5177, Accuracy: 78.48%
Epoch 10/100, Loss: 0.5088, Accuracy: 78.72%
Epoch 11/100, Loss: 0.4995, Accuracy: 80.19%
Epoch 12/100, Loss: 0.4915, Accuracy: 80.49%
Epoch 13/100, Loss: 0.4845, Accuracy: 80.87%
Epoch 14/100, Loss: 0.4782, Accuracy: 81.25%
Epoch 15/100, Loss: 0.4724, Accuracy: 81.36%
Epoch 16/100, Loss: 0.4665, Accuracy: 81.85%
Epoch 17/100, Loss: 0.4603, Accuracy: 81.96%
Epoch 18/100, Loss: 0.4555, Accuracy: 83.12%
Epoch 19/100, Loss: 0.4514, Accuracy: 82.55%
Epoch 20/100, Loss: 0.4471, Accuracy: 83.12%
Epoch 21/100, Loss: 0.4439, Accuracy: 83.15%
Epoch 22/100, Loss: 0.4396, Accuracy: 83.10%
Epoch 23/100, Loss: 0.4357, Accuracy: 83.56%
Epoch 24/100, Loss: 0.4317, Accuracy: 83.37%
Epoch 25/100, Loss: 0.4285, Accuracy: 83.83%
Epoch 26/100, Loss: 0.4259, Accuracy: 84.21%
Epoch 27/100, Loss: 0.4225, Accuracy: 84.46%
Epoch 28/100, Loss: 0.4205, Accuracy: 84.10%
Epoch 29/100, Loss: 0.4176, Accuracy: 84.62%
Epoch 30/100, Loss: 0.4155, Accuracy: 84.18%
Epoch 31/100, Loss: 0.4131, Accuracy: 84.78%
Epoch 32/100, Loss: 0.4096, Accuracy: 84.73%
Epoch 33/100, Loss: 0.4075, Accuracy: 84.73%
Epoch 34/100, Loss: 0.4061, Accuracy: 84.92%
Epoch 35/100, Loss: 0.4040, Accuracy: 84.43%
Epoch 36/100, Loss: 0.4009, Accuracy: 85.14%
Epoch 37/100, Loss: 0.3994, Accuracy: 84.92%
Epoch 38/100, Loss: 0.3976, Accuracy: 84.95%
Epoch 39/100, Loss: 0.3953, Accuracy: 85.84%
Epoch 40/100, Loss: 0.3950, Accuracy: 85.76%
Epoch 41/100, Loss: 0.3919, Accuracy: 85.41%
Epoch 42/100, Loss: 0.3905, Accuracy: 85.68%
Epoch 43/100, Loss: 0.3877, Accuracy: 85.79%
Epoch 44/100, Loss: 0.3866, Accuracy: 85.68%
Epoch 45/100, Loss: 0.3853, Accuracy: 86.30%
Epoch 46/100, Loss: 0.3842, Accuracy: 85.82%
Epoch 47/100, Loss: 0.3817, Accuracy: 85.90%
Epoch 48/100, Loss: 0.3807, Accuracy: 86.01%
Epoch 49/100, Loss: 0.3792, Accuracy: 85.92%
Epoch 50/100, Loss: 0.3780, Accuracy: 86.22%
Epoch 51/100, Loss: 0.3767, Accuracy: 86.03%
Epoch 52/100, Loss: 0.3748, Accuracy: 86.25%
Epoch 53/100, Loss: 0.3741, Accuracy: 86.44%
Epoch 54/100, Loss: 0.3724, Accuracy: 86.41%
Epoch 55/100, Loss: 0.3713, Accuracy: 86.82%
Epoch 56/100, Loss: 0.3703, Accuracy: 86.55%
Epoch 57/100, Loss: 0.3692, Accuracy: 86.79%
Epoch 58/100, Loss: 0.3679, Accuracy: 86.63%
Epoch 59/100, Loss: 0.3661, Accuracy: 87.04%
Epoch 60/100, Loss: 0.3659, Accuracy: 86.39%
Epoch 61/100, Loss: 0.3637, Accuracy: 86.79%
Epoch 62/100, Loss: 0.3631, Accuracy: 86.79%
Epoch 63/100, Loss: 0.3616, Accuracy: 86.88%
Epoch 64/100, Loss: 0.3612, Accuracy: 86.93%
Epoch 65/100, Loss: 0.3605, Accuracy: 87.07%
Epoch 66/100, Loss: 0.3591, Accuracy: 87.61%
Epoch 67/100, Loss: 0.3569, Accuracy: 87.17%
Epoch 68/100, Loss: 0.3565, Accuracy: 87.69%
Epoch 69/100, Loss: 0.3561, Accuracy: 87.47%
Epoch 70/100, Loss: 0.3556, Accuracy: 87.31%
Epoch 71/100, Loss: 0.3544, Accuracy: 87.74%
Epoch 72/100, Loss: 0.3534, Accuracy: 87.39%
Epoch 73/100, Loss: 0.3521, Accuracy: 87.66%
Epoch 74/100, Loss: 0.3514, Accuracy: 87.45%
Epoch 75/100, Loss: 0.3507, Accuracy: 87.50%
Epoch 76/100, Loss: 0.3496, Accuracy: 87.72%
Epoch 77/100, Loss: 0.3490, Accuracy: 87.36%
Epoch 78/100, Loss: 0.3483, Accuracy: 87.66%
Epoch 79/100, Loss: 0.3467, Accuracy: 87.80%
Epoch 80/100, Loss: 0.3471, Accuracy: 87.85%
Epoch 81/100, Loss: 0.3456, Accuracy: 87.83%
Epoch 82/100, Loss: 0.3459, Accuracy: 87.80%
Epoch 83/100, Loss: 0.3443, Accuracy: 88.07%
Epoch 84/100, Loss: 0.3438, Accuracy: 88.10%
Epoch 85/100, Loss: 0.3425, Accuracy: 87.85%
Epoch 86/100, Loss: 0.3423, Accuracy: 87.83%
Epoch 87/100, Loss: 0.3410, Accuracy: 88.15%
Epoch 88/100, Loss: 0.3392, Accuracy: 88.15%
Epoch 89/100, Loss: 0.3398, Accuracy: 87.93%
Epoch 90/100, Loss: 0.3395, Accuracy: 87.99%
Epoch 91/100, Loss: 0.3390, Accuracy: 87.96%
Epoch 92/100, Loss: 0.3380, Accuracy: 88.26%
Epoch 93/100, Loss: 0.3369, Accuracy: 88.53%
Epoch 94/100, Loss: 0.3356, Accuracy: 88.21%
Epoch 95/100, Loss: 0.3359, Accuracy: 88.26%
Epoch 96/100, Loss: 0.3353, Accuracy: 88.32%
Epoch 97/100, Loss: 0.3341, Accuracy: 88.42%
Epoch 98/100, Loss: 0.3338, Accuracy: 88.51%
Epoch 99/100, Loss: 0.3326, Accuracy: 88.45%
Epoch 100/100, Loss: 0.3323, Accuracy: 88.40%
Test Accuracy: 88.38%
Training MLP...
Epoch 1/100, Loss: 0.6673, Accuracy: 60.84%
Epoch 2/100, Loss: 0.6571, Accuracy: 61.36%
Epoch 3/100, Loss: 0.6443, Accuracy: 61.60%
Epoch 4/100, Loss: 0.6264, Accuracy: 62.55%
Epoch 5/100, Loss: 0.5947, Accuracy: 67.09%
Epoch 6/100, Loss: 0.5574, Accuracy: 73.61%
Epoch 7/100, Loss: 0.5164, Accuracy: 78.70%
Epoch 8/100, Loss: 0.4799, Accuracy: 79.78%
Epoch 9/100, Loss: 0.4493, Accuracy: 82.09%
Epoch 10/100, Loss: 0.4273, Accuracy: 82.64%
Epoch 11/100, Loss: 0.4106, Accuracy: 83.59%
Epoch 12/100, Loss: 0.3931, Accuracy: 83.99%
Epoch 13/100, Loss: 0.3806, Accuracy: 84.70%
Epoch 14/100, Loss: 0.3753, Accuracy: 84.76%
Epoch 15/100, Loss: 0.3621, Accuracy: 85.05%
Epoch 16/100, Loss: 0.3565, Accuracy: 86.20%
Epoch 17/100, Loss: 0.3464, Accuracy: 86.47%
Epoch 18/100, Loss: 0.3424, Accuracy: 86.49%
Epoch 19/100, Loss: 0.3344, Accuracy: 86.66%
Epoch 20/100, Loss: 0.3344, Accuracy: 86.49%
Epoch 21/100, Loss: 0.3269, Accuracy: 86.98%
Epoch 22/100, Loss: 0.3191, Accuracy: 87.50%
Epoch 23/100, Loss: 0.3152, Accuracy: 87.28%
Epoch 24/100, Loss: 0.3138, Accuracy: 87.64%
Epoch 25/100, Loss: 0.3116, Accuracy: 87.58%
Epoch 26/100, Loss: 0.3038, Accuracy: 87.28%
Epoch 27/100, Loss: 0.3031, Accuracy: 87.72%
Epoch 28/100, Loss: 0.2981, Accuracy: 88.26%
Epoch 29/100, Loss: 0.3032, Accuracy: 87.99%
Epoch 30/100, Loss: 0.2982, Accuracy: 88.04%
Epoch 31/100, Loss: 0.2918, Accuracy: 88.37%
Epoch 32/100, Loss: 0.2959, Accuracy: 87.96%
Epoch 33/100, Loss: 0.2850, Accuracy: 89.02%
Epoch 34/100, Loss: 0.2857, Accuracy: 88.99%
Epoch 35/100, Loss: 0.2825, Accuracy: 88.56%
Epoch 36/100, Loss: 0.2861, Accuracy: 88.42%
Epoch 37/100, Loss: 0.2793, Accuracy: 89.29%
Epoch 38/100, Loss: 0.2827, Accuracy: 88.64%
Epoch 39/100, Loss: 0.2823, Accuracy: 88.61%
Epoch 40/100, Loss: 0.2812, Accuracy: 88.89%
Epoch 41/100, Loss: 0.2807, Accuracy: 88.61%
Epoch 42/100, Loss: 0.2847, Accuracy: 88.67%
Epoch 43/100, Loss: 0.2783, Accuracy: 89.57%
Epoch 44/100, Loss: 0.2744, Accuracy: 89.38%
Epoch 45/100, Loss: 0.2667, Accuracy: 89.43%
Epoch 46/100, Loss: 0.2708, Accuracy: 89.21%
Epoch 47/100, Loss: 0.2729, Accuracy: 89.67%
Epoch 48/100, Loss: 0.2636, Accuracy: 89.70%
Epoch 49/100, Loss: 0.2664, Accuracy: 89.89%
Epoch 50/100, Loss: 0.2654, Accuracy: 89.76%
Epoch 51/100, Loss: 0.2672, Accuracy: 89.43%
Epoch 52/100, Loss: 0.2653, Accuracy: 89.38%
Epoch 53/100, Loss: 0.2651, Accuracy: 89.70%
Epoch 54/100, Loss: 0.2665, Accuracy: 89.65%
Epoch 55/100, Loss: 0.2569, Accuracy: 89.86%
Epoch 56/100, Loss: 0.2598, Accuracy: 90.00%
Epoch 57/100, Loss: 0.2626, Accuracy: 90.27%
Epoch 58/100, Loss: 0.2566, Accuracy: 89.76%
Epoch 59/100, Loss: 0.2576, Accuracy: 89.70%
Epoch 60/100, Loss: 0.2543, Accuracy: 90.35%
Epoch 61/100, Loss: 0.2601, Accuracy: 89.73%
Epoch 62/100, Loss: 0.2553, Accuracy: 90.22%
Epoch 63/100, Loss: 0.2541, Accuracy: 90.14%
Epoch 64/100, Loss: 0.2569, Accuracy: 90.08%
Epoch 65/100, Loss: 0.2555, Accuracy: 90.14%
Epoch 66/100, Loss: 0.2553, Accuracy: 90.03%
Epoch 67/100, Loss: 0.2449, Accuracy: 90.38%
Epoch 68/100, Loss: 0.2554, Accuracy: 89.89%
Epoch 69/100, Loss: 0.2504, Accuracy: 90.49%
Epoch 70/100, Loss: 0.2477, Accuracy: 90.08%
Epoch 71/100, Loss: 0.2524, Accuracy: 89.81%
Epoch 72/100, Loss: 0.2515, Accuracy: 89.78%
Epoch 73/100, Loss: 0.2456, Accuracy: 90.65%
Epoch 74/100, Loss: 0.2511, Accuracy: 89.54%
Epoch 75/100, Loss: 0.2522, Accuracy: 90.35%
Epoch 76/100, Loss: 0.2427, Accuracy: 90.57%
Epoch 77/100, Loss: 0.2461, Accuracy: 90.27%
Epoch 78/100, Loss: 0.2445, Accuracy: 90.60%
Epoch 79/100, Loss: 0.2517, Accuracy: 90.14%
Epoch 80/100, Loss: 0.2401, Accuracy: 90.65%
Epoch 81/100, Loss: 0.2468, Accuracy: 90.82%
Epoch 82/100, Loss: 0.2515, Accuracy: 89.95%
Epoch 83/100, Loss: 0.2393, Accuracy: 90.41%
Epoch 84/100, Loss: 0.2438, Accuracy: 90.68%
Epoch 85/100, Loss: 0.2392, Accuracy: 90.82%
Epoch 86/100, Loss: 0.2466, Accuracy: 90.27%
Epoch 87/100, Loss: 0.2394, Accuracy: 90.65%
Epoch 88/100, Loss: 0.2445, Accuracy: 90.30%
Epoch 89/100, Loss: 0.2409, Accuracy: 90.52%
Epoch 90/100, Loss: 0.2506, Accuracy: 90.38%
Epoch 91/100, Loss: 0.2386, Accuracy: 90.49%
Epoch 92/100, Loss: 0.2401, Accuracy: 90.79%
Epoch 93/100, Loss: 0.2373, Accuracy: 90.68%
Epoch 94/100, Loss: 0.2446, Accuracy: 90.65%
Epoch 95/100, Loss: 0.2381, Accuracy: 90.38%
Epoch 96/100, Loss: 0.2480, Accuracy: 89.97%
Epoch 97/100, Loss: 0.2388, Accuracy: 90.98%
Epoch 98/100, Loss: 0.2413, Accuracy: 90.60%
Epoch 99/100, Loss: 0.2356, Accuracy: 90.82%
Epoch 100/100, Loss: 0.2415, Accuracy: 90.49%
Test Accuracy: 91.10%
Models saved as 'log_reg.pth' and 'mlp.pth'
